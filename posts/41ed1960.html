<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>基于 ollama 从零部署大语言模型 | Sora33</title><meta name="author" content="Sora33"><meta name="copyright" content="Sora33"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="前言环境介绍：使用的 Python 版本为 3.12.4 且需要科学上网，对电脑性能有一定需求（但也没那么离谱） 相信不少人已经体验过 Ai 带来的便利了，甚至在工作上使用 Ai 加以辅助。本文是对那些对 Ai 产生兴趣且希望在自己的设备上实际运行大语言模型的人而准备，希望可以通过这篇文章来让更多人认识和了解 Ai。本文是基于 ollama 创建和部署大语言模型，下面就开始进入正文。 ollama">
<meta property="og:type" content="article">
<meta property="og:title" content="基于 ollama 从零部署大语言模型">
<meta property="og:url" content="http://33sora.com/posts/41ed1960.html">
<meta property="og:site_name" content="Sora33">
<meta property="og:description" content="前言环境介绍：使用的 Python 版本为 3.12.4 且需要科学上网，对电脑性能有一定需求（但也没那么离谱） 相信不少人已经体验过 Ai 带来的便利了，甚至在工作上使用 Ai 加以辅助。本文是对那些对 Ai 产生兴趣且希望在自己的设备上实际运行大语言模型的人而准备，希望可以通过这篇文章来让更多人认识和了解 Ai。本文是基于 ollama 创建和部署大语言模型，下面就开始进入正文。 ollama">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/241219/wallhaven-l8ljyr_1920x1080.png">
<meta property="article:published_time" content="2024-09-27T05:33:47.000Z">
<meta property="article:modified_time" content="2025-01-22T01:09:02.150Z">
<meta property="article:author" content="Sora33">
<meta property="article:tag" content="大语言模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/241219/wallhaven-l8ljyr_1920x1080.png"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://33sora.com/posts/41ed1960.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')
          
          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 8 || hour >= 19
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"1QY6GKFJYT","apiKey":"ff7af72210f9015cb3c5205c3a3824e2","indexName":"dev_home","hitsPerPage":6,"languages":{"input_placeholder":"搜索文章","hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":500,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '基于 ollama 从零部署大语言模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><link rel="stylesheet" href="/css/rightMenu.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="pokeball-loading"><div class="pokeball" id="normal"></div><div class="pokeball" id="great"></div><div class="pokeball" id="ultra"></div><div class="pokeball" id="master"></div><div class="pokeball" id="safari"></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = 'auto'
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="web_bg" style="background-image: url(https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/0817/wallhaven-g7mpj7_1920x1080.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/nayuta.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">65</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">71</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-calendar"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-wheelchair-alt"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/241219/wallhaven-l8ljyr_1920x1080.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/nayuta.jpg" alt="Logo"><span class="site-name">Sora33</span></a><a class="nav-page-title" href="/"><span class="site-name">基于 ollama 从零部署大语言模型</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-calendar"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-wheelchair-alt"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">基于 ollama 从零部署大语言模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2024-09-27T05:33:47.000Z" title="发表于 2024-09-27 13:33:47">2024-09-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%95%99%E7%A8%8B/">教程</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">2.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:300,&quot;messagePrev&quot;:&quot;距离最后更新已经过了&quot;,&quot;messageNext&quot;:&quot;天，文章内容可能已过时，如果有问题，欢迎留言反馈&quot;,&quot;postUpdate&quot;:&quot;2025-01-22 09:09:02&quot;}" hidden></div><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>环境介绍：使用的 Python 版本为 3.12.4 且需要科学上网，对电脑性能有一定需求（但也没那么离谱）</p>
<p>相信不少人已经体验过 Ai 带来的便利了，甚至在工作上使用 Ai 加以辅助。本文是对那些对 Ai 产生兴趣且希望在自己的设备上实际运行大语言模型的人而准备，希望可以通过这篇文章来让更多人认识和了解 Ai。本文是基于 ollama 创建和部署大语言模型，下面就开始进入正文。</p>
<h2 id="ollama是什么？"><a href="#ollama是什么？" class="headerlink" title="ollama是什么？"></a>ollama 是什么？</h2><p>ollama 是一个管理大语言模型的工具，帮助我们在本地快速创建、部署、使用大语言模型。它本身并不是一个大语言模型！还有一个名词 <strong>llama</strong>，它跟 ollama 很像，但是区别可就大了。llama 是由 Meta 开发的大语言模型，所以我们可以使用 ollama 加载 llama 这样的大语言模型。</p>
<h2 id="安装ollama"><a href="#安装ollama" class="headerlink" title="安装ollama"></a>安装 ollama</h2><p>进入 <a target="_blank" rel="noopener" href="https://ollama.com/">ollama</a> 官网，点击下载按钮，安装到自己电脑上即可。安装完成后可以使用 <code>ollama -v</code> 测试是否安装成功</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409271356536.png" alt="image-20240927135638565"></p>
<h2 id="使用ollama运行llama3-2模型"><a href="#使用ollama运行llama3-2模型" class="headerlink" title="使用ollama运行llama3.2模型"></a>使用 ollama 运行 llama3.2 模型</h2><p>我们先简单启动一个模型进行测试，我们以 Meta 最新的 llama3.2 的 1b 模型为例，执行下面命令可以自动安装并启动 llama3.2 模型。</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run llama3.2:1b</span><br></pre></td></tr></tbody></table></figure>

<p>在模型启动后，我们可以进行一些简单的对话，不过最好使用英文，因为 llama3 目前还不支持中文，简单的体验完成后，我们继续准备通过 ollama 部署各种开源模型到本地。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409271402872.png" alt="image-20240927140250811"></p>
<p>ollama 常用命令：</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看模型列表</span></span><br><span class="line">ollama list</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看目前启动的模型</span></span><br><span class="line">ollama ps</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除模型</span></span><br><span class="line">ollama rm ${模型名}</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动模型</span></span><br><span class="line">ollama run ${模型名}</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">停止模型</span></span><br><span class="line">ollama stop ${模型名}</span><br></pre></td></tr></tbody></table></figure>

<h2 id="在Huggingface上选择模型"><a href="#在Huggingface上选择模型" class="headerlink" title="在Huggingface上选择模型"></a>在 Huggingface 上选择模型</h2><p>Huggingface 是一个集开源工具、模型库、数据集为一体的平台，提供了 transformers 库、Hugging Face Hub 等功能，其中 Hub 是一个类似于 GitHub 的代码托管平台，我们可以在上面下载开源的模型、数据集使用。本次我们就要通过 Huggingface 下载模型，首先进入 <a target="_blank" rel="noopener" href="https://huggingface.co/models">Hugging face 模型页</a></p>
<p>以大语言对话模型为例，在左侧筛选，仅选中 <strong>Text Generation</strong></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409271422850.png" alt="image-20240927142250801" style="zoom:50%;">

<p>继续在搜索框中输入 Qwen，选择 <strong>Qwen/Qwen2.5-Coder-7B-Instruct</strong>，也就是阿里的通义千问模型，Coder 是编程特化型，和基础版相比提升了编程性能。下面是模型首页 <a target="_blank" rel="noopener" href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Huggingface-Qwen2.5-7B</a></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409301106916.png" alt="image-20240930110600817"></p>
<h2 id="下载模型"><a href="#下载模型" class="headerlink" title="下载模型"></a>下载模型</h2><p>现在模型的主要存储格式主要有 <strong>gguf</strong> 和 <strong>safetensors</strong>，gguf 采用了二进制格式编码，优化了数据结构和资源占用，可以直接通过 ollama 加载。safetensors 则是未经过量化、更侧重于文件加载速度和安全的一种格式。</p>
<p>如果一个模型提供了 gguf 格式，我们可以直接下载下来，编写启动脚本完成模型创建，而 safetensors 格式则需要先将模型转变为 gguf 格式后再编写脚本创建。不管哪种格式，我们都需要先将模型下载下来，下面我们来看两种格式不同的下载方式。</p>
<h3 id="gguf："><a href="#gguf：" class="headerlink" title="gguf："></a>gguf：</h3><p>我们进入模型详情页后，点击 <strong>Files and versions</strong>，可以看到具体的文件，而且都有不一样的后缀，例如 Q3、Q4 等，这代表量化精度，这里先不介绍，后面会细说，目前先记住数字越大，精度越高。我们选择一个模型，点击后面的下载按钮下载下来（只需要下载一个，不同文件之间只有精度区分）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409271539867.png" alt="image-20240927153941811"></p>
<h3 id="safetensors："><a href="#safetensors：" class="headerlink" title="safetensors："></a>safetensors：</h3><p>-–</p>
<p><strong>更新于 2025-01-13：</strong></p>
<p>更新 safetensors 模型的量化方式，因为 llama.cpp 项目进行了大更改，编译方式改为了 cmake</p>
<p><strong>更新于 2025-01-22：</strong></p>
<p>更新 windows 下的编译方式，从 WSL 虚拟环境编译变更为 本机编译 </p>
<p>-–</p>
<p>和 gguf 相比，safetensors 要麻烦许多，我们需要借助 <code>llama.cpp</code> 项目，将 safetensors 转为 gguf 格式。</p>
<p>项目地址：<a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp">llama.cpp</a></p>
<p>创建一个 python 项目，创建好虚拟环境（这里我 Python 的版本为 3.12.4，最好保持一致）</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409271549546.png" alt="image-20240927154951477"></p>
<p>进入项目，拉取 <code>llama.cpp</code> 项目，因为我们需要借助 <code>cmake</code> 编译项目，所以需要提前安装，mac 可以通过 brew 命令安装：<code>brew install cmake</code></p>
<p>windows 安装：</p>
<p>因为 windows 上的 cmake 还需要配合 c++ 和 c 编译器，所以比较麻烦，这里我分为 2 个部分，第一个是安装 cmake ，第二个是安装 w64devkit 并配置环境变量</p>
<p>cmake 的安装很简单，直接去官网下载安装就可以，注意安装的时候把添加到环境变量的框打上，这样就不用手动配置 cmake 的环境变量了。</p>
<p>w64devkit 则是一个整合了很多编译器的项目，其中就包含 c++ 和 c，并且是一个开源项目：<a target="_blank" rel="noopener" href="https://github.com/skeeto/w64devkit/releases/tag/v2.0.0">w64devkit</a>，这里我提供的是目前最新的 2.0 版本，下载后是一个 exe 文件，点开选择解压路径（注意最好不要包含中文），例如我解压到了 c 盘根目录，就是 <code>C:\w64devkit </code>，下面我们配置环境变量，在系统的 PATH 中新增一条：<code>C:\w64devkit\bin\bin</code>，注意路径需要跟你解压的路径一致</p>
<p>下面就可以开始拉取 llama.cpp 项目并编译</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">拉取 llama.cpp 项目到本地</span></span><br><span class="line">https://github.com/ggerganov/llama.cpp.git</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装依赖</span></span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">对于linux、mac的编译</span></span><br><span class="line">cmake ./</span><br><span class="line">cmake --build . --config Release</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">对于windows的编译，注意后面的换成你的w64devkit路径</span></span><br><span class="line">cmake -G "MinGW Makefiles" -D CMAKE_MAKE_PROGRAM=C:/w64devkit/bin/make.exe ./</span><br><span class="line">cmake --build . --config Release</span><br></pre></td></tr></tbody></table></figure>

<p>到这里，llama.cpp 工具的初始化就完成了。我们继续下载对应的大语言模型文件。这里我们需要用到 Hugging face 的官方工具 <code>huggingface-cli</code>。因为后面需要登陆，我们需要先注册一个 Huggingface 账号，并在个人设置内生成一个 token</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409271609516.png" alt="image-20240927160907443"></p>
<p>现在我们可以继续输入下面 2 个命令，下载 huggingface_hub 工具并登陆</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下载huggingface_hub工具</span></span><br><span class="line">pip install huggingface_hub</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">登陆（会提示你输入token）</span></span><br><span class="line">huggingface-cli login</span><br></pre></td></tr></tbody></table></figure>

<p>登陆成功后，我们就可以成功下载模型了，具体的下载的命令如下，这里对参数进行解释：<code>download</code>：后面跟要下载的模型，注意需要包含模型前缀，例如示例中的 Qwen/，一般直接在网页复制模型名就行。<code>resume-download</code>：如果下载中断，下次可以继续下载。<code>local-dir</code>：指定具体的下载路径</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli download --resume-download Qwen/Qwen2.5-Coder-7B-Instruct --local-dir /Users/sora33/download</span><br></pre></td></tr></tbody></table></figure>

<p>PS： <strong>这里因为更新的原因，所以该部分后面大家看到的演示目录和自己的不一样是正常的，更新后只会让大家把 <code>llama.cpp</code> 项目拉下来</strong></p>
<p>模型下载完成后，我们将下载的东西移动到 <code>llama.cpp</code> 项目下的 model 文件夹下，方便我们后续操作（如果没有 model 文件夹，创建一个即可）。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409271617226.png" alt="image-20240927161707053" style="zoom:50%;">

<p>开始对模型进行转换，通过 <code>llama.cpp</code> 根目录下的 <strong>convert_hf_to_gguf.py</strong> 文件，转换对象是 model 文件夹，指定输出文件类型为 f16（16 位浮点数）：</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python ./convert_hf_to_gguf.py ./model --outtype f16 </span><br></pre></td></tr></tbody></table></figure>

<p>我们可以在 model 文件夹内看到结果文件，格式为 gguf，模型格式转换成功</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409271640603.png" alt="image-20240927164025506" style="zoom:50%;">

<blockquote>
<p>PS：这里可能会遇到 <code>ChatGLM4Tokenizer._pad() got an unexpected keyword argument 'padding_side'</code> 这个错误，原因是部分模型可能还不适配最新版本，所以我们需要把 <code>transformers</code> 降级，执行下面语句降级即可：</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip uninstall transformers -y</span><br><span class="line">pip install transformers==4.34.0</span><br></pre></td></tr></tbody></table></figure>

<p>​		若提示 tiktoken 未安装，直接使用 <code>pip install tiktoken</code> 安装即可：</p>
</blockquote>
<p>最后我们需要量化模型，先来简单介绍一下量化模型的作用。量化模型是通过降低浮点数来减少模型大小、加速推理和降低内存占用的过程，量化后模型精度会下降，但是效率和内存占用会贬低。量化等级按照从低到高排序（f16 精度最高，q2_K 最低）：</p>
<ul>
<li><code>q2_K</code></li>
<li><code>q3_K</code></li>
<li><code>q3_K_S</code></li>
<li><code>q3_K_M</code></li>
<li><code>q3_K_L</code></li>
<li><code>q4_0</code>（受到推崇的）</li>
<li><code>q4_1</code></li>
<li><code>q4_K</code></li>
<li><code>q4_K_S</code></li>
<li><code>q4_K_M</code></li>
<li><code>q5_0</code></li>
<li><code>q5_1</code></li>
<li><code>q5_K</code></li>
<li><code>q5_K_S</code></li>
<li><code>q5_K_M</code></li>
<li><code>q6_K</code></li>
<li><code>q8_0</code></li>
<li><code>f16</code></li>
</ul>
<p>我们的量化命令如下，调用 <code>llama.cpp</code> bin 目录下的 <code>llama-quantize</code>，输入 <code>Model-7.6B-F16.gguf</code>、输出 <code>qwen7.6B_q4_0.gguf</code>，量化粒度为 <code>q4_0</code>。</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/llama-quantize ./model/Model-7.6B-F16.gguf qwen7.6B_q4_0.gguf q4_0</span><br></pre></td></tr></tbody></table></figure>

<p>量化完成后，结果文件会输出到根目录下，这里我又移动到了 model 里面做对比。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409271701093.png" alt="image-20240927170118053" style="zoom:50%;">

<p>同时可以看到，在 q4_0 粒度的量化下，直接少了 10 个 G，压缩效率非常好</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409271700413.png" alt="image-20240927170055345" style="zoom:50%;">

<h2 id="编写启动脚本"><a href="#编写启动脚本" class="headerlink" title="编写启动脚本"></a>编写启动脚本</h2><p>现在我们拥有了 gguf 格式的文件，开始编写大模型的启动脚本 <strong>modelfile</strong>。新建一个 txt，FROM 后面跟我们模型的具体路径，PARAMETER 设置温度系数，值越高回答的随机性越大。SYSTEM 设置系统消息，用来定义角色或上下文，这里我们置空。完成后将文件保存并命名为 qwen.txt。（名字可以自定义）</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM /Users/sora33/PythonCode/qwen/ollama/model/qwen7.6B_q4_0.gguf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">set</span> the temperature to 1 [higher is more creative, lower is more coherent]</span></span><br><span class="line">PARAMETER temperature 1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">set</span> the system message</span></span><br><span class="line">SYSTEM """"""</span><br></pre></td></tr></tbody></table></figure>

<p>随后我们通过 <strong>ollama create</strong> 命令创建模型，模型命名 qwen，读取配置文件 qwen.txt。</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama create qwen -f qwen.txt</span><br></pre></td></tr></tbody></table></figure>

<p>命令执行后，等待片刻，模型会创建完毕，如下：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409301116217.png" alt="image-20240930111648125"></p>
<p>通过 run 命令启动模型并对话测试。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409301135812.png" alt="image-20240930113522721"></p>
<h2 id="open-webui可视化界面"><a href="#open-webui可视化界面" class="headerlink" title="open-webui可视化界面"></a>open-webui 可视化界面</h2><p>目前为止，大模型已经创建完成并可以使用，但没有可视化页面只能在命令行内调用多少有点简陋。GitHub 上的一个开源项目 <a target="_blank" rel="noopener" href="https://github.com/open-webui/open-webui">open-webui</a> 可以完美适配 ollama，web 页面接入 ollama 也很简单，下面来具体操作一下，因为版本兼容问题，这里我们只以 Docker 为例，当然也可以通过 Python 启动（Python 版本必须需要在 3.11，这里我因为是 3.12 所以不方便演示）</p>
<h3 id="Python："><a href="#Python：" class="headerlink" title="Python："></a>Python：</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Python</span></span><br><span class="line">pip install open-webui</span><br><span class="line">open-webui serve</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">访问</span></span><br><span class="line">http://localhost:8080</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Docker："><a href="#Docker：" class="headerlink" title="Docker："></a>Docker：</h3><p>我们不需要改动任何东西，只需要知道将原本的 8080 端口改到了 3000，如果 3000 跟你本地有冲突，也可以换成别的。执行这个 Docker 命令完成 open-webui 的创建。</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</span><br></pre></td></tr></tbody></table></figure>

<p>访问 localhost:3000，进入主页面，会提示你登陆，这里简单注册一下就可以，数据都在你本地。之后左上角选定模型，就可以在 web 页面使用自己已经创建的大模型了</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/%E6%95%99%E5%AD%A6%E7%9B%AE%E5%BD%95/202409301145796.png" alt="image-20240930114535631"></p>
<h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p>以上就是如何使用 ollama 配合 Huggingface 平台加载和运行大语言模型，特别是对于 safetensors 格式的模型。如果在流程上遇到什么问题，欢迎各位进行留言。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://33sora.com/">Sora33</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://33sora.com/posts/41ed1960.html">http://33sora.com/posts/41ed1960.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://33sora.com" target="_blank">Sora33</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">大语言模型</a></div><div class="post-share"><div class="social-share" data-image="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/241219/wallhaven-l8ljyr_1920x1080.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/89e44969.html" title="hexo &amp; butterfly 升级与注意要点"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/241219/wallhaven-rr913w_1920x1080.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">hexo & butterfly 升级与注意要点</div></div><div class="info-2"><div class="info-item-1">前言 &amp; 准备PS：不管是再怎么熟练，操作前一定要把原博客备份好！！！ 最近想换个加载动画，但是在跟着弄的时候发现，博客不管是框架本身还是主题都已经跟不上了，所以打算升级一下。下面放一下升级的版本信息： Hexo：hexo 当前版本是 Current 列的信息，这里我全部更到了最新的 Latest 版本  Butterfly：我用的主题是 butterfly，在 hexo 的根目录下用 hexo cl 可以看到版本，旧的是 4.3.1，我升级到了 5.2.2  Hexo 升级Hexo 的升级非常简单，进入博客根目录，先看是否有可用更新 1234# 使用 `version` 查看当前的 hexo 版本hexo version# 查看是否有可升级版本，为空则表示当前为最新npm outdated  如果有最新版本，修改博客根目录的 package.json 文件内的版本号，下图为示例：  之后再通过命令行安装即可 1234# 安装项目依赖npm install --save# 检查是否升级成功hexo version  Butterfly...</div></div></div></a><a class="pagination-related" href="/posts/ef6e6a13.html" title="个人对设计模式的理解与实践 (持续缓更)"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/wallhaven-x8kzjo_1920x1080.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">个人对设计模式的理解与实践 (持续缓更)</div></div><div class="info-2"><div class="info-item-1">设计模式设计模式是什么？设计模式是我们对问题所提出的解决方案，就像一个个蓝图，通过对问题的一些综合考虑，采用最合适的设计方案来解决问题。就像一个工具箱，我们要看具体的情况，来决定使用哪把工具。那么设计模式是如何诞生的呢，设计模式最开始也是一个解决方案，只不过这个方案在各种项目中得到了验证。最终得到认可，是前辈们一个个试验，一步一个坑踩过来，最终被后人们整理，收纳，所归类的出的一种新领域 设计模式的优点 提高我们的思维能力和设计能力 使程序的设计变得标准化、流程化，增强开发效率 对代码来说，提高了可读性和复用性以及可扩展性  设计模式的六大原则 单一职责： 一个类应该只有一个会引起它变化的原因，也就是一个类只负责一个职责	 开闭原则： 对扩展开放，对修改关闭 里氏代换原则： 子类应该可以替换父类对象，并保持逻辑不变 依赖倒转原则： 抽象不依赖细节，细节依赖于抽象。也就是对接口编程，不要直接使用实现类 接口隔离原则： 不应该强迫一个类实现它不需要的方法，而是使用多个精细化的接口 迪米特法则：...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/a39037a1.html" title="在 java 中使用 deepseek 并接入联网搜索和知识库"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/241219/wallhaven-v9852l_1920x1080.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">在 java 中使用 deepseek 并接入联网搜索和知识库</div></div><div class="info-2"><div class="info-item-1">前言当前 AI 技术生态以 Python 为主导，这几天在研究用 Java 搭建知识库使用，最终都避不开 Python，于是打算记录下结果，目前是有 2 个方案，第一个方案是 在 Python 中使用 embedding 嵌入模型，完成数据向量化与向量搜索，推荐使用这个方案，简单也方便。第二个方案是不使用 embedding 嵌入模型，使用 es 来完成向量存储，但仍需要 Python 来完成数据的向量化。 本文分为三部分，第一部分是接入 deepseek-r1，第二部分是接入联网搜索，第三部分是使用自建知识库（两个实现方案），知识库为可选功能，并且实现起来也挺麻烦，不需要的可以直接看前两部分即可。 同时，本次的代码也已经放在了 GitHub 上，deepseek-java 前置准备首先介绍一下本次的开发环境： Java17 + SpringBoot 3.3.2 Python 3.11 deepseek 的 APIkeys（在官网上买就可以了） tavily（搜索引擎，通过这个实现联网搜索，在第二部分会具体说） 项目依赖我们需要一个 SpringBoot...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/nayuta.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Sora33</div><div class="author-info-description">未来无限可能</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">65</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">71</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="http://wpa.qq.com/msgrd?v=3&amp;uin=2097665736&amp;site=qq&amp;menu=yes" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="/img/sora33QR.jpg" target="_blank" title="VX"><i class="fab fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Java/自宅警备员/nayuta单推人<br>本站主要记录自己自学的一些技术,欢迎各位一起留言讨论。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ollama%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">ollama 是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85ollama"><span class="toc-number">3.</span> <span class="toc-text">安装 ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8ollama%E8%BF%90%E8%A1%8Cllama3-2%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text">使用 ollama 运行 llama3.2 模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8Huggingface%E4%B8%8A%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text">在 Huggingface 上选择模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.</span> <span class="toc-text">下载模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gguf%EF%BC%9A"><span class="toc-number">6.1.</span> <span class="toc-text">gguf：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#safetensors%EF%BC%9A"><span class="toc-number">6.2.</span> <span class="toc-text">safetensors：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E5%86%99%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC"><span class="toc-number">7.</span> <span class="toc-text">编写启动脚本</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#open-webui%E5%8F%AF%E8%A7%86%E5%8C%96%E7%95%8C%E9%9D%A2"><span class="toc-number">8.</span> <span class="toc-text">open-webui 可视化界面</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Python%EF%BC%9A"><span class="toc-number">8.1.</span> <span class="toc-text">Python：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Docker%EF%BC%9A"><span class="toc-number">8.2.</span> <span class="toc-text">Docker：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9D%9F%E8%AF%AD"><span class="toc-number">9.</span> <span class="toc-text">结束语</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/5bf17f0.html" title="使用 CUDA 部署 LLM、TTS、ASR 三种类型的开源模型"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/0817/wallhaven-8ordoj_1920x1080.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="使用 CUDA 部署 LLM、TTS、ASR 三种类型的开源模型"/></a><div class="content"><a class="title" href="/posts/5bf17f0.html" title="使用 CUDA 部署 LLM、TTS、ASR 三种类型的开源模型">使用 CUDA 部署 LLM、TTS、ASR 三种类型的开源模型</a><time datetime="2025-05-28T01:47:30.000Z" title="发表于 2025-05-28 09:47:30">2025-05-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a39037a1.html" title="在 java 中使用 deepseek 并接入联网搜索和知识库"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/241219/wallhaven-v9852l_1920x1080.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="在 java 中使用 deepseek 并接入联网搜索和知识库"/></a><div class="content"><a class="title" href="/posts/a39037a1.html" title="在 java 中使用 deepseek 并接入联网搜索和知识库">在 java 中使用 deepseek 并接入联网搜索和知识库</a><time datetime="2025-03-06T08:15:40.000Z" title="发表于 2025-03-06 16:15:40">2025-03-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/dcc14389.html" title="写了一个 Java 中过滤实体类字段的小项目"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/0131/wallhaven-weee76_1920x1080.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="写了一个 Java 中过滤实体类字段的小项目"/></a><div class="content"><a class="title" href="/posts/dcc14389.html" title="写了一个 Java 中过滤实体类字段的小项目">写了一个 Java 中过滤实体类字段的小项目</a><time datetime="2024-12-24T08:12:50.000Z" title="发表于 2024-12-24 16:12:50">2024-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/16a3faad.html" title="如何将自己的 jar 包发布到 Maven 中央仓库？"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/wallhaven-m9ozwm_1920x1080.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="如何将自己的 jar 包发布到 Maven 中央仓库？"/></a><div class="content"><a class="title" href="/posts/16a3faad.html" title="如何将自己的 jar 包发布到 Maven 中央仓库？">如何将自己的 jar 包发布到 Maven 中央仓库？</a><time datetime="2024-12-19T02:42:15.000Z" title="发表于 2024-12-19 10:42:15">2024-12-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/89e44969.html" title="hexo &amp; butterfly 升级与注意要点"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/241219/wallhaven-rr913w_1920x1080.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="hexo &amp; butterfly 升级与注意要点"/></a><div class="content"><a class="title" href="/posts/89e44969.html" title="hexo &amp; butterfly 升级与注意要点">hexo &amp; butterfly 升级与注意要点</a><time datetime="2024-12-09T07:10:21.000Z" title="发表于 2024-12-09 15:10:21">2024-12-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://minaseinori.oss-cn-hongkong.aliyuncs.com/blog/241219/wallhaven-l8ljyr_1920x1080.png);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Sora33</div><div class="footer_custom_text"><a target="_blank" rel="noopener" href="http://beian.miit.gov.cn"><span>晋ICP备2022007182号</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="fa-solid fa-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="fa-solid fa-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="fa-solid fa-arrow-rotate-right"></i></div><div class="rightMenu-item" id="menu-home"><i class="fa-solid fa-house"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" href="/archives/"><i class="fa-solid fa-archive"></i><span>文章归档</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="fa-solid fa-folder-open"></i><span>文章分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="fa-solid fa-tags"></i><span>文章标签</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.isShuoshuo
  const option = null

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://sprightly-liger-8d5ebf.netlify.app/.netlify/functions/twikoo',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://sprightly-liger-8d5ebf.netlify.app/.netlify/functions/twikoo',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/js/rightMenu.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div></body></html>